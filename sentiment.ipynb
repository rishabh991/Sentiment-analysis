{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supportlib\n",
      "  Downloading https://test-files.pythonhosted.org/packages/c7/e8/a44bb606fca2603f0c79e8593fe0f6f1626dee5bad5177afb9ee260fd223/supportlib-0.1.0-py3-none-any.whl\n",
      "Installing collected packages: supportlib\n",
      "Successfully installed supportlib-0.1.0\n"
     ]
    }
   ],
   "source": [
    "#download dataset from kaggle in csv format\n",
    "!pip3 install -i https://test.pypi.org/simple/ supportlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.colab\n",
      "\u001b[31m  Could not find a version that satisfies the requirement google.colab (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for google.colab\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imdb_master.csv.zip'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download('https://storage.googleapis.com/kaggle-datasets/9914/14036/imdb_master.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1547275066&Signature=oLJbzz5f0%2BHhjZ8MSkCRrq%2FEKsAwpKxfrmF6LnrCz0zWk%2BT3MeLBNOi0t7lO60rC%2FRDCKTvm5H%2F9Sl942ESEb6icE8wJdcF1jgsy6UcB7ulkpnbXDYuWbXYcLyRPFcWSh3a8ZVD4z%2Bl8Eswq5En7h%2FR%2FglLw4DMj3aV42e%2B4pqynt9B1zrTjYDwnI2QmkEV6PYhc2yHmu%2F%2FvAfXGNZQJtFVBf05z%2BFN7nzFElwoqG%2BsqiUl7mELegHKcgckhqvQ8QPrVNwQ%2BYTtjPUU76OzwL4SZqc77maQyybXz%2BYtaYSl%2B0K5wektQWNRdpT1dhEEOw30v6Fz3Rl4EUGjniP7zTA%3D%3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip data\n",
    "sl.zipextract('./imdb_master.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading imdb_master.csv\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./imdb_master.csv',encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sanjaymoto75/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sanjaymoto75/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data = data.sort_values(\"type\")\n",
    "data = data.drop(['Unnamed: 0','file'],axis=1)\n",
    "data.columns = [\"type\",\"review\",\"sentiment\"]\n",
    "data['sentiment'] = data['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "data['type'] = data['type'].map({'test':0 , 'train':1})\n",
    "data = data.dropna()\n",
    "review = data['review']\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "def cleantext(rev):\n",
    "  rev = re.sub(r'[^a-zA-Z]',' ',rev)\n",
    "  rev = rev.lower()\n",
    "  rev = rev.split()\n",
    "  rev = [word for word in rev if not word in set(stopwords.words('english'))]\n",
    "  rev = [lemmatizer.lemmatize(token) for token in rev]\n",
    "  rev = \" \".join(rev)\n",
    "  return rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.apply(lambda x: cleantext(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0        mr costner dragged movie far longer necessary ...\n",
       "16671    aside fact movie filmed mostly rockport beauti...\n",
       "16670    well done photography sound music performance ...\n",
       "16669    thought love letter pretty good movie certain ...\n",
       "16668    film half bad may little long time carried alo...\n",
       "16667    movie sicky sweet cutesy romantic comedy kind ...\n",
       "16666    sir played lear time tonight remember opening ...\n",
       "16665    rita hayworth rusty parker cover girl du jour ...\n",
       "16664    thought movie pretty good really enjoyed viewe...\n",
       "16663    love letter one time favorite book naturally s...\n",
       "16672    one better feel good film kate capshaw leading...\n",
       "16662    love letter somewhat pleasant low key romantic...\n",
       "16660    along line comedy error mistaken affection tra...\n",
       "16659    back bought chinese box set fist north star ca...\n",
       "16658    watch film like peter watkins privilege story ...\n",
       "16657    curiosity patience finally see controversial f...\n",
       "16656    think film made year born think people still c...\n",
       "16655    said film banned u since released since inform...\n",
       "16654    definitely outstanding musical great young sta...\n",
       "16653    punishment park brilliant piece cinema shot so...\n",
       "16652    contains spoiler br br peter watkins film one ...\n",
       "16661    movie sweet main character lady sensitive diff...\n",
       "16651    punishment park pseudo documentary made peter ...\n",
       "16673    definitely girl movie husband found utterly bo...\n",
       "16675    love letter starring kate capshaw tom everett ...\n",
       "16695    pixote directed barely shred sentimentality ye...\n",
       "16694    speak relatively one see hector babenco pixote...\n",
       "16693    story take place street sao paoulo brazil youn...\n",
       "16692    trust excellent accurate junagadh review film ...\n",
       "16691    movie outstanding acting marilia p ra stunning...\n",
       "                               ...                        \n",
       "49922    rented domino whim even knowing inspired true ...\n",
       "49921    totally surprised comment forum many review th...\n",
       "49972    spoiler br br really minority one liked movie ...\n",
       "49973    say loved vanishing point seen original pretty...\n",
       "49974    start since movie remake classic rating lowere...\n",
       "49975    agree post comedy drama leaned little much tow...\n",
       "49901    absolutely wonderful drama ro top notch highly...\n",
       "49989    classic many great dialog scene nobody miss ni...\n",
       "49988    thought movie hysterical watched many time rec...\n",
       "49987    saw movie recently really liked surprised crie...\n",
       "49986    really interesting movie action movie comedy m...\n",
       "49985    chance see screening movie recently believe th...\n",
       "49984    loved screen adaptation stone angel stayed tru...\n",
       "49990    hillarious funny brook movie ever seen watch w...\n",
       "49983    moved story going something similar parent rea...\n",
       "49981    saw film winnipeg recently appropriate given l...\n",
       "49980    surprised much enjoyed thoughtfully delivered ...\n",
       "49979    movie basically human relation interaction mai...\n",
       "49978    saw movie last night waiting age age released ...\n",
       "49977    canadian director kari skogland film adaptatio...\n",
       "49976    movie show federal pig person government power...\n",
       "49982    perhaps one canadian read book high school tho...\n",
       "49991    life stink parody life death happiness depress...\n",
       "49992    kind film want see glass wine fire foot requir...\n",
       "49993    read comment film judging average rating see u...\n",
       "49998    christmas together actually came time raised j...\n",
       "49997    amazed movie others average star lower crappy ...\n",
       "49996    plot wretched unbelievable twist however chemi...\n",
       "49995    seeing vote average pretty low fact clerk vide...\n",
       "49994    life stink step mel brook production star rich...\n",
       "Name: review, Length: 50000, dtype: object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 10s 249us/step - loss: 0.3368 - acc: 0.8521 - val_loss: 0.3628 - val_acc: 0.8466\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 9s 220us/step - loss: 0.2260 - acc: 0.9131 - val_loss: 0.3448 - val_acc: 0.8560\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 9s 220us/step - loss: 0.2018 - acc: 0.9238 - val_loss: 0.3416 - val_acc: 0.8581\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 9s 219us/step - loss: 0.1883 - acc: 0.9307 - val_loss: 0.3610 - val_acc: 0.8490\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 9s 219us/step - loss: 0.1778 - acc: 0.9363 - val_loss: 0.3760 - val_acc: 0.8444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c3a071588>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , CuDNNLSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(data['review'])\n",
    "maxlen = 200\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y_train = data['sentiment']\n",
    "\n",
    "embed_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(CuDNNLSTM(32, return_sequences = True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"elu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01,decay=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Keira Knightley does sublime justice to her real-life subject in this lithe, topical ode to female empowerment.\"]\n",
    "text[0] = cleantext(text[0])\n",
    "list_tokenized = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "maxlen = 200\n",
    "X_test = pad_sequences(list_tokenized, maxlen=maxlen)\n",
    "import numpy as np\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.rint(y_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5067255571170448\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
